import streamlit as st
from langchain_community.llms import Ollama
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
import time


llm = Ollama(model="llama3:8b")

def main():
    st.title("CircuitSage-Asistente TÃ©cnico")

    bot_name = "CircuitSage"
    bot_description = f"""Eres un asistente virtual especializado en resolver problemas tÃ©cnicos de laptops y computadoras de sobremesa solamente. Te llamas {bot_name}, respondes preguntas con respuestas detalladas. AdemÃ¡s, debes preguntar al usuario acorde al contexto del chat y tambiÃ©n preguntar al usuario para obtener una respuesta mÃ¡s detallada. Solo te presentarÃ¡s con un hola y preguntando al usuario quÃ© se le ofrece o cuÃ¡l es su problema. Cualquier tema que no estÃ© relacionado con el hardware de las computadoras y laptops descÃ¡rtalo de forma contundente."""

    if "chat_history" not in st.session_state:
        st.session_state["chat_history"] = []

    prompt_template = ChatPromptTemplate.from_messages(
        [
            ("system", bot_description),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
        ]
    )

    chain = prompt_template | llm

    user_input = st.text_input("Escribe tu pregunta:", key="user_input")

    if st.button("Enviar"):
        if user_input.lower() == "adios":
            st.stop()
        else:
            with st.spinner("Generando respuesta, por favor espera..."):
                start_time = time.time()
                try:
                    response = chain.invoke({"input": user_input, "chat_history": st.session_state["chat_history"]})
                    st.session_state["chat_history"].append(HumanMessage(content=user_input))
                    st.session_state["chat_history"].append(AIMessage(content=response))
                except Exception as e:
                    st.error("Error al generar la respuesta. AsegÃºrate de que Ollama estÃ¡ funcionando correctamente.")
                    st.error(str(e))
                elapsed_time = time.time() - start_time
                if elapsed_time > 60:
                    st.warning("La generaciÃ³n de la respuesta estÃ¡ tardando mÃ¡s de lo esperado.")

    chat_display = "\n".join(
        f"ğŸ¦§Yo: {msg.content}" if isinstance(msg, HumanMessage) else f"ğŸ”§{bot_name}: {msg.content}"
        for msg in st.session_state["chat_history"]
    )

    st.text_area("Chat", value=chat_display, height=400, key="chat_area", disabled=True)

if __name__ == '__main__':
    main()
